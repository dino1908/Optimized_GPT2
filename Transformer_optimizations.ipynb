{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46fc69e-d0dc-4d7f-a855-15c713518058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033b86e7-539c-49d7-abdb-e99a471b5e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0+cu128'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7176a93-80d1-407e-a8c8-70ac0878d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bdd4765-0ee7-4b27-b17c-8f590cdbb9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,  \n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"attention_type\": \"flash_v2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c123c2e-0e75-4294-8e1c-6e4f6b1f0290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4197f38-405e-4077-a23a-f727e23f04f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 24 08:35:44 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.144                Driver Version: 570.144        CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5090        On  |   00000000:0A:00.0 Off |                  N/A |\n",
      "|  0%   28C    P5             20W /  575W |       4MiB /  32607MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d2f70ef-49df-422e-9818-cb7c478af8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Loaded Successfully. Length of book is 1115393\n"
     ]
    }
   ],
   "source": [
    "#Load file\n",
    "f = \"shakespeare.txt\"\n",
    "with open(f, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "print(f\"Book Loaded Successfully. Length of book is {len(text_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae9c8c0-32a8-455c-9fec-9cb874221cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Data is 892314\n",
      "Length of Val Data is 223079\n"
     ]
    }
   ],
   "source": [
    "#Divide Test & Val data\n",
    "train_ratio = 0.8\n",
    "split = int(0.8*len(text_data))\n",
    "train_data = text_data[:split]\n",
    "val_data = text_data[split:]\n",
    "print(f\"Length of Train Data is {len(train_data)}\")\n",
    "print(f\"Length of Val Data is {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad90ba-980f-442f-abe5-87d50210a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b038ae29-e568-4c57-95ef-1ae410b70360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=32, max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = DatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "705b9a63-517d-4d6e-83ad-19b7b300ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Loaders\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    max_length=GPT_CONFIG[\"context_length\"],\n",
    "    stride=GPT_CONFIG[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    " )\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=32,\n",
    "    max_length=GPT_CONFIG[\"context_length\"],\n",
    "    stride=GPT_CONFIG[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc943ac-5da5-45e0-8408-5b1c0a15850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Multi head attention class\n",
    "\n",
    "class MultiAttn(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, n_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % n_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_out // n_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_values = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_values(x)\n",
    "\n",
    "        keys = keys.view(b, n_tokens, self.n_heads, self.head_dim)\n",
    "        queries = queries.view(b, n_tokens, self.n_heads, self.head_dim)\n",
    "        values = values.view(b, n_tokens, self.n_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        attn_score = queries@keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:n_tokens, :n_tokens]\n",
    "        attn_score.masked_fill(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_score/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = (attn_weights@values).transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(b,n_tokens,self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6795a487-07f9-497f-81f5-f538bc2be885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
    "\n",
    "class FlashAttentionV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, n_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % n_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_out // n_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_values = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = drop_out\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_values(x)\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            keys = keys.view(b, n_tokens, self.n_heads, self.head_dim).transpose(1,2)\n",
    "            queries = queries.view(b, n_tokens, self.n_heads, self.head_dim).transpose(1,2)\n",
    "            values = values.view(b, n_tokens, self.n_heads, self.head_dim).transpose(1,2)\n",
    "    \n",
    "            #Context Manager to select Attention type\n",
    "            \n",
    "            context_vec = F.scaled_dot_product_attention(queries, \n",
    "                                                         keys, \n",
    "                                                         values,\n",
    "                                                         dropout_p=self.dropout if self.training else 0.0,\n",
    "                                                         is_causal=True)\n",
    "            \n",
    "            \n",
    "            context_vec = context_vec.transpose(1,2).contiguous().view(b, n_tokens, self.d_out)\n",
    "            context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e28e097-2389-4bd6-96fa-038f01c7d6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0+cu128'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbf61307-f8d7-457d-af58-bfe4cf2e0ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFactory:\n",
    "    \"\"\"\n",
    "    Factory class to create attention mechanisms.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_attention(attention_name):\n",
    "        attention_map = {\n",
    "            \"multihead\": MultiAttn,\n",
    "            #\"flash\": FlashAttention,\n",
    "            \"flash_v2\": FlashAttentionV2\n",
    "        }\n",
    "\n",
    "        return attention_map.get(attention_name.lower(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52fdfbf0-c6f3-487e-8e0b-53f69c304136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        #print(f\"x: {torch.get_device(x)} | x_norm: {torch.get_device(x_norm)}| scale: {torch.get_device(self.scale)}|shift: {torch.get_device(self.shift)}\")\n",
    "        return self.scale*x_norm + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49f80bd0-eef6-4a73-be47-351c12d1e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6de0baad-7ccf-4b2e-9fdc-f53c7869ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attention_type = cfg[\"attention_type\"]\n",
    "        self.attn = AttentionFactory.get_attention(cfg[\"attention_type\"])(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            drop_out=cfg[\"drop_rate\"],\n",
    "            n_heads=cfg[\"n_heads\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "291aa69d-65c8-4e0e-81f1-d3ac1ae10824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        #self.device = device\n",
    "        self.tok_embed = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_embed = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_block = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        b, seq_len = in_idx.shape\n",
    "        tok_emb = self.tok_embed(in_idx)\n",
    "        pos_emb = self.pos_embed(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = x\n",
    "\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_block(x)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da22666a-3544-4d0f-8e5e-981dcaa8d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a00ee07b-22d3-4f9d-a440-0a1092ddee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature = 0.0, top_k = None, eos_id=None):\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.inference_mode():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:,-1,:]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:,-1]\n",
    "            logits = torch.where(logits<min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "132ed44a-a168-4d70-b08b-2a1ffa0d7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    model.to(device)\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits = model(input_batch)\n",
    "        loss = nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(dataloader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(dataloader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i<num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device=device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5baa6ca-601c-4c9e-94a4-6ced91287aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_dataloader, val_dataloader, num_epochs, optimizer, \n",
    "                    tokenizer, eval_freq, eval_iter, start_context, device):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X, y in train_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(X, y, model, device=device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += X.numel()\n",
    "            global_step += 1\n",
    "\n",
    "        \n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                model, train_dataloader, val_dataloader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "        if device==\"cuda\" and epoch==5:\n",
    "            print(torch.cuda.memory_summary(device=device, abbreviated=True))\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b044d103-705e-48da-8469-07e96c7d9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device=device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device=device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2979c830-2a99-4f49-9fd0-86cf71653570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_embed.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27e16b00-9325-4b7b-bb3c-57c185ea732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")\n",
    "model = GPTModel(GPT_CONFIG).to(device)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a0464b-fd0b-4cdb-9349-9943274b56a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash_v2 Attention\n",
      "Ep 1 (Step 000000): Train loss 9.449, Val loss 9.439\n",
      "Ep 1 (Step 000005): Train loss 7.900, Val loss 7.911\n",
      "Ep 1 (Step 000010): Train loss 6.721, Val loss 6.861\n",
      "Ep 1 (Step 000015): Train loss 6.300, Val loss 6.484\n",
      "Ep 1 (Step 000020): Train loss 6.151, Val loss 6.381\n",
      "Ep 1 (Step 000025): Train loss 6.013, Val loss 6.315\n",
      "Ep 1 (Step 000030): Train loss 5.974, Val loss 6.229\n",
      "Every effort moves you,                                                 \n",
      "Ep 2 (Step 000035): Train loss 5.889, Val loss 6.185\n",
      "Ep 2 (Step 000040): Train loss 5.870, Val loss 6.166\n",
      "Ep 2 (Step 000045): Train loss 5.831, Val loss 6.148\n",
      "Ep 2 (Step 000050): Train loss 5.814, Val loss 6.113\n",
      "Ep 2 (Step 000055): Train loss 5.765, Val loss 6.074\n",
      "Ep 2 (Step 000060): Train loss 5.724, Val loss 6.049\n",
      "Every effort moves you                                                  \n",
      "Ep 3 (Step 000065): Train loss 5.696, Val loss 6.017\n",
      "Ep 3 (Step 000070): Train loss 5.660, Val loss 6.044\n",
      "Ep 3 (Step 000075): Train loss 5.522, Val loss 5.988\n",
      "Ep 3 (Step 000080): Train loss 5.458, Val loss 5.904\n",
      "Ep 3 (Step 000085): Train loss 5.380, Val loss 5.787\n",
      "Ep 3 (Step 000090): Train loss 5.247, Val loss 5.700\n",
      "Ep 3 (Step 000095): Train loss 5.119, Val loss 5.649\n",
      "Every effort moves you,                                                 \n",
      "Ep 4 (Step 000100): Train loss 5.053, Val loss 5.590\n",
      "Ep 4 (Step 000105): Train loss 5.060, Val loss 5.559\n",
      "Ep 4 (Step 000110): Train loss 4.861, Val loss 5.516\n",
      "Ep 4 (Step 000115): Train loss 4.869, Val loss 5.478\n",
      "Ep 4 (Step 000120): Train loss 4.820, Val loss 5.428\n",
      "Ep 4 (Step 000125): Train loss 4.729, Val loss 5.374\n",
      "Every effort moves you, And I'll be a man, And, And, And, And, And, And, And, And, And, And, And in the king, And in the king\n",
      "Ep 5 (Step 000130): Train loss 4.672, Val loss 5.361\n",
      "Ep 5 (Step 000135): Train loss 4.656, Val loss 5.326\n",
      "Ep 5 (Step 000140): Train loss 4.556, Val loss 5.313\n",
      "Ep 5 (Step 000145): Train loss 4.469, Val loss 5.296\n",
      "Ep 5 (Step 000150): Train loss 4.430, Val loss 5.277\n",
      "Ep 5 (Step 000155): Train loss 4.427, Val loss 5.254\n",
      "Every effort moves you have been   And, And, And, And, And, And so much the king, And, And, And, And, And, And so much more than the queen,\n",
      "Ep 6 (Step 000160): Train loss 4.368, Val loss 5.211\n",
      "Ep 6 (Step 000165): Train loss 4.311, Val loss 5.208\n",
      "Ep 6 (Step 000170): Train loss 4.270, Val loss 5.192\n",
      "Ep 6 (Step 000175): Train loss 4.236, Val loss 5.190\n",
      "Ep 6 (Step 000180): Train loss 4.185, Val loss 5.183\n",
      "Ep 6 (Step 000185): Train loss 4.159, Val loss 5.168\n",
      "Ep 6 (Step 000190): Train loss 4.099, Val loss 5.157\n",
      "Every effort moves you, And, And, And I'll be not a man.         IUS: IUS: IUS: IUS: IUS: IUS: I\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   2546 MiB |  10368 MiB |   9163 GiB |   9161 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   2546 MiB |  10368 MiB |   9163 GiB |   9161 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   2494 MiB |  10308 MiB |   9139 GiB |   9137 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  11522 MiB |  11522 MiB |  11522 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    905 MiB |   1911 MiB |   3266 GiB |   3265 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     649    |     885    |  724438    |  723789    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     649    |     885    |  724438    |  723789    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     230    |     230    |     230    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      62    |      75    |  291774    |  291712    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Ep 7 (Step 000195): Train loss 4.092, Val loss 5.163\n",
      "Ep 7 (Step 000200): Train loss 3.994, Val loss 5.155\n",
      "Ep 7 (Step 000205): Train loss 3.984, Val loss 5.147\n",
      "Ep 7 (Step 000210): Train loss 3.884, Val loss 5.127\n",
      "Ep 7 (Step 000215): Train loss 3.846, Val loss 5.134\n",
      "Ep 7 (Step 000220): Train loss 3.825, Val loss 5.118\n",
      "Every effort moves you are you.    CALUS: IUS: IUS: IUS: IUS: IUS: IUS: IUS: IUS: IUS: IUS\n",
      "Ep 8 (Step 000225): Train loss 3.789, Val loss 5.127\n",
      "Ep 8 (Step 000230): Train loss 3.711, Val loss 5.119\n",
      "Ep 8 (Step 000235): Train loss 3.695, Val loss 5.130\n",
      "Ep 8 (Step 000240): Train loss 3.649, Val loss 5.133\n",
      "Ep 8 (Step 000245): Train loss 3.561, Val loss 5.112\n",
      "Ep 8 (Step 000250): Train loss 3.490, Val loss 5.115\n",
      "Ep 8 (Step 000255): Train loss 3.469, Val loss 5.101\n",
      "Every effort moves you, And with the king.  DUKE OF YORK: I will be the king.  DUKE OF YORK: I will not, I will be the king.  DUKE OF YORK: \n",
      "Ep 9 (Step 000260): Train loss 3.396, Val loss 5.133\n",
      "Ep 9 (Step 000265): Train loss 3.349, Val loss 5.125\n",
      "Ep 9 (Step 000270): Train loss 3.299, Val loss 5.133\n",
      "Ep 9 (Step 000275): Train loss 3.194, Val loss 5.135\n",
      "Ep 9 (Step 000280): Train loss 3.171, Val loss 5.127\n",
      "Ep 9 (Step 000285): Train loss 3.106, Val loss 5.132\n",
      "Every effort moves you, And with the world's daughter'st, And in the king, And in the king, And in the king, And in the earth, And in the earth, And in the king, And in\n",
      "Ep 10 (Step 000290): Train loss 3.017, Val loss 5.153\n",
      "Ep 10 (Step 000295): Train loss 2.957, Val loss 5.171\n",
      "Ep 10 (Step 000300): Train loss 2.886, Val loss 5.158\n",
      "Ep 10 (Step 000305): Train loss 2.842, Val loss 5.200\n",
      "Ep 10 (Step 000310): Train loss 2.719, Val loss 5.218\n",
      "Ep 10 (Step 000315): Train loss 2.664, Val loss 5.223\n",
      "Every effort moves you are.  BUCKINGHAM: I am the king of your grace  BUCKINGHAM: I do him.  GLOUCESTER: I am the king; And, And in the\n",
      "Ep 11 (Step 000320): Train loss 2.647, Val loss 5.223\n",
      "Ep 11 (Step 000325): Train loss 2.575, Val loss 5.264\n",
      "Ep 11 (Step 000330): Train loss 2.469, Val loss 5.286\n",
      "Ep 11 (Step 000335): Train loss 2.350, Val loss 5.320\n",
      "Ep 11 (Step 000340): Train loss 2.345, Val loss 5.311\n",
      "Ep 11 (Step 000345): Train loss 2.299, Val loss 5.355\n",
      "Ep 11 (Step 000350): Train loss 2.166, Val loss 5.338\n",
      "Every effort moves you have begun.  MENENIUS: IUS: IUS: IUS: I would not.  CORIOLANUS: I do?  CORIOLANUS: IUS\n",
      "Ep 12 (Step 000355): Train loss 2.083, Val loss 5.391\n",
      "Ep 12 (Step 000360): Train loss 2.017, Val loss 5.438\n",
      "Ep 12 (Step 000365): Train loss 1.944, Val loss 5.482\n",
      "Ep 12 (Step 000370): Train loss 1.880, Val loss 5.521\n",
      "Ep 12 (Step 000375): Train loss 1.805, Val loss 5.530\n",
      "Ep 12 (Step 000380): Train loss 1.726, Val loss 5.546\n",
      "Every effort moves you have begun to To do; and then you do not.  MENENIUS: I do beseech you, good sir.  MENENIUS: IUS: I do beseech you,\n",
      "Ep 13 (Step 000385): Train loss 1.680, Val loss 5.584\n",
      "Ep 13 (Step 000390): Train loss 1.625, Val loss 5.647\n",
      "Ep 13 (Step 000395): Train loss 1.556, Val loss 5.685\n",
      "Ep 13 (Step 000400): Train loss 1.488, Val loss 5.713\n",
      "Ep 13 (Step 000405): Train loss 1.392, Val loss 5.745\n",
      "Ep 13 (Step 000410): Train loss 1.348, Val loss 5.760\n",
      "Ep 13 (Step 000415): Train loss 1.295, Val loss 5.744\n",
      "Every effort moves you now what?  MENENIUS: I am a Roman, I have seen a Roman and one that I'll do not well. I am not be so.  CORIOLANUS: IOLAN\n",
      "Ep 14 (Step 000420): Train loss 1.255, Val loss 5.839\n",
      "Ep 14 (Step 000425): Train loss 1.146, Val loss 5.870\n",
      "Ep 14 (Step 000430): Train loss 1.129, Val loss 5.932\n",
      "Ep 14 (Step 000435): Train loss 1.055, Val loss 5.933\n",
      "Ep 14 (Step 000440): Train loss 1.019, Val loss 5.970\n",
      "Ep 14 (Step 000445): Train loss 0.936, Val loss 5.964\n",
      "Every effort moves you well as the people Of what he will have heard the queen of the king was it in the king Of goodly; and there.  LEONTES: What is so!  LEONTES: Good queen and\n",
      "Ep 15 (Step 000450): Train loss 0.895, Val loss 6.069\n",
      "Ep 15 (Step 000455): Train loss 0.843, Val loss 6.110\n",
      "Ep 15 (Step 000460): Train loss 0.792, Val loss 6.130\n",
      "Ep 15 (Step 000465): Train loss 0.722, Val loss 6.154\n",
      "Ep 15 (Step 000470): Train loss 0.715, Val loss 6.161\n",
      "Ep 15 (Step 000475): Train loss 0.644, Val loss 6.153\n",
      "Every effort moves you had rather refuse The noble lords, we have wrought To have we have cushions by our own.  Lord: You have more come, my lord, And so much in the city be done.  HENRY\n",
      "Ep 16 (Step 000480): Train loss 0.611, Val loss 6.223\n",
      "Ep 16 (Step 000485): Train loss 0.569, Val loss 6.291\n",
      "Ep 16 (Step 000490): Train loss 0.552, Val loss 6.336\n",
      "Ep 16 (Step 000495): Train loss 0.497, Val loss 6.385\n",
      "Ep 16 (Step 000500): Train loss 0.499, Val loss 6.369\n",
      "Ep 16 (Step 000505): Train loss 0.438, Val loss 6.421\n",
      "Ep 16 (Step 000510): Train loss 0.406, Val loss 6.428\n",
      "Every effort moves you're well.  Second Lord: What's have, I'll take him?  Third Citizen: If it with him, what's in his purpose.  First Citizen: Well, but the people, sir, but\n",
      "Ep 17 (Step 000515): Train loss 0.390, Val loss 6.508\n",
      "Ep 17 (Step 000520): Train loss 0.375, Val loss 6.559\n",
      "Ep 17 (Step 000525): Train loss 0.352, Val loss 6.567\n",
      "Ep 17 (Step 000530): Train loss 0.314, Val loss 6.589\n",
      "Ep 17 (Step 000535): Train loss 0.313, Val loss 6.609\n",
      "Ep 17 (Step 000540): Train loss 0.280, Val loss 6.591\n",
      "Every effort moves you had rather refuse The noble man have mercy.  CORIOLANUS: IOLANUS: You have been so; and You have seen me You have heard it to To have it concerns me To\n",
      "Ep 18 (Step 000545): Train loss 0.253, Val loss 6.660\n",
      "Ep 18 (Step 000550): Train loss 0.232, Val loss 6.696\n",
      "Ep 18 (Step 000555): Train loss 0.229, Val loss 6.752\n",
      "Ep 18 (Step 000560): Train loss 0.214, Val loss 6.787\n",
      "Ep 18 (Step 000565): Train loss 0.190, Val loss 6.806\n",
      "Ep 18 (Step 000570): Train loss 0.173, Val loss 6.782\n",
      "Ep 18 (Step 000575): Train loss 0.165, Val loss 6.788\n",
      "Every effort moves you had been so.  QUEEN: The heavens of heaven forbid our lord the stars Should so with civil and uncivil arms The bloody parliament shall this young prince; And makes us rich, That I make them,\n",
      "Ep 19 (Step 000580): Train loss 0.157, Val loss 6.849\n",
      "Ep 19 (Step 000585): Train loss 0.144, Val loss 6.873\n",
      "Ep 19 (Step 000590): Train loss 0.146, Val loss 6.903\n",
      "Ep 19 (Step 000595): Train loss 0.125, Val loss 6.925\n",
      "Ep 19 (Step 000600): Train loss 0.130, Val loss 6.978\n",
      "Ep 19 (Step 000605): Train loss 0.113, Val loss 6.969\n",
      "Every effort moves you had rather refuse The offer of an hundred thousand crowns Than Bolingbroke's return.  DUKE OF YORK: I do meet thy father, And leave not thy state to do beseech you,\n",
      "Ep 20 (Step 000610): Train loss 0.103, Val loss 7.018\n",
      "Ep 20 (Step 000615): Train loss 0.096, Val loss 7.032\n",
      "Ep 20 (Step 000620): Train loss 0.089, Val loss 7.094\n",
      "Ep 20 (Step 000625): Train loss 0.085, Val loss 7.040\n",
      "Ep 20 (Step 000630): Train loss 0.091, Val loss 7.088\n",
      "Ep 20 (Step 000635): Train loss 0.081, Val loss 7.099\n",
      "Every effort moves you here receive her father: The which will I; not all so much for love Than on the part of York, But sever'd for his goodness.  BENVOLIO: A call'd he evade us there,\n",
      "Ep 21 (Step 000640): Train loss 0.073, Val loss 7.110\n",
      "Ep 21 (Step 000645): Train loss 0.066, Val loss 7.160\n",
      "Ep 21 (Step 000650): Train loss 0.058, Val loss 7.155\n",
      "Ep 21 (Step 000655): Train loss 0.061, Val loss 7.213\n",
      "Ep 21 (Step 000660): Train loss 0.061, Val loss 7.202\n",
      "Ep 21 (Step 000665): Train loss 0.059, Val loss 7.183\n",
      "Ep 21 (Step 000670): Train loss 0.052, Val loss 7.241\n",
      "Every effort moves you gave leave to save and Sir John.  ISABELLA: And have you nuns no farther privileges?  FRANCISCA: So, I may.  ISABELLA: You may-morrow:\n",
      "Ep 22 (Step 000675): Train loss 0.049, Val loss 7.267\n",
      "Ep 22 (Step 000680): Train loss 0.050, Val loss 7.314\n",
      "Ep 22 (Step 000685): Train loss 0.044, Val loss 7.305\n",
      "Ep 22 (Step 000690): Train loss 0.040, Val loss 7.278\n",
      "Ep 22 (Step 000695): Train loss 0.045, Val loss 7.317\n",
      "Ep 22 (Step 000700): Train loss 0.040, Val loss 7.327\n",
      "Every effort moves you; What other pleasure can he is content.  CORIOLANUS: IOLANUS: I'll not have't.  CORIOLANUS: The senate-ring: I have publish'd\n",
      "Ep 23 (Step 000705): Train loss 0.036, Val loss 7.334\n",
      "Ep 23 (Step 000710): Train loss 0.036, Val loss 7.355\n",
      "Ep 23 (Step 000715): Train loss 0.034, Val loss 7.361\n",
      "Ep 23 (Step 000720): Train loss 0.034, Val loss 7.389\n",
      "Ep 23 (Step 000725): Train loss 0.029, Val loss 7.403\n",
      "Ep 23 (Step 000730): Train loss 0.033, Val loss 7.416\n",
      "Ep 23 (Step 000735): Train loss 0.028, Val loss 7.431\n",
      "Every effort moves you gave leave?  CAPULET: My sword, I say! Old Montague is no health with me.  MONTAGUE: Thou art the cause, before, And breathed his foot,--Hold me not\n",
      "Ep 24 (Step 000740): Train loss 0.029, Val loss 7.494\n",
      "Ep 24 (Step 000745): Train loss 0.029, Val loss 7.465\n",
      "Ep 24 (Step 000750): Train loss 0.027, Val loss 7.467\n",
      "Ep 24 (Step 000755): Train loss 0.027, Val loss 7.472\n",
      "Ep 24 (Step 000760): Train loss 0.026, Val loss 7.426\n",
      "Ep 24 (Step 000765): Train loss 0.024, Val loss 7.469\n",
      "Every effort moves you gave leave to save both are undone.  VIRGILIA: No, at a word, madam; indeed, I must not. I wish you much mirth.  VALERIA: Well, then\n",
      "Ep 25 (Step 000770): Train loss 0.024, Val loss 7.504\n",
      "Ep 25 (Step 000775): Train loss 0.028, Val loss 7.543\n",
      "Ep 25 (Step 000780): Train loss 0.023, Val loss 7.561\n",
      "Ep 25 (Step 000785): Train loss 0.022, Val loss 7.494\n",
      "Ep 25 (Step 000790): Train loss 0.021, Val loss 7.501\n",
      "Ep 25 (Step 000795): Train loss 0.020, Val loss 7.515\n",
      "Every effort moves you now?  MISTRESS OVERDONE: I am too sure of it: and it is for getting Madam Julietta with child.  LUCIO: Believe me, this may be: he\n",
      "Ep 26 (Step 000800): Train loss 0.020, Val loss 7.536\n",
      "Ep 26 (Step 000805): Train loss 0.022, Val loss 7.548\n",
      "Ep 26 (Step 000810): Train loss 0.022, Val loss 7.597\n",
      "Ep 26 (Step 000815): Train loss 0.022, Val loss 7.578\n",
      "Ep 26 (Step 000820): Train loss 0.020, Val loss 7.615\n",
      "Ep 26 (Step 000825): Train loss 0.019, Val loss 7.599\n",
      "Ep 26 (Step 000830): Train loss 0.018, Val loss 7.568\n",
      "Every effort moves you gave leave to To whose high will we bound to do To save King Richard and labour to eject him hence The other part, which am we may blow of my kinsmen.  LEONTES: You know you are beg\n",
      "Ep 27 (Step 000835): Train loss 0.018, Val loss 7.619\n",
      "Ep 27 (Step 000840): Train loss 0.017, Val loss 7.680\n",
      "Ep 27 (Step 000845): Train loss 0.019, Val loss 7.610\n",
      "Ep 27 (Step 000850): Train loss 0.018, Val loss 7.674\n",
      "Ep 27 (Step 000855): Train loss 0.016, Val loss 7.636\n",
      "Ep 27 (Step 000860): Train loss 0.017, Val loss 7.641\n",
      "Every effort moves you gave leave to save both are undone.  VIRGILIA: No, at a word, madam; I must go with us. I wish you much mirth.  VALERIA: Well, then\n",
      "Ep 28 (Step 000865): Train loss 0.014, Val loss 7.676\n",
      "Ep 28 (Step 000870): Train loss 0.017, Val loss 7.707\n"
     ]
    }
   ],
   "source": [
    "#Training starts here!!\n",
    "\n",
    "start_time = time.time()\n",
    "device = device\n",
    "\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "print(f\"Using {GPT_CONFIG['attention_type']} Attention\")\n",
    "\n",
    "num_epochs = 30\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model, train_dataloader=train_loader, val_dataloader=val_loader, optimizer=optimizer, device=device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "\n",
    "print(\"Training completed successfully! Existing training block\")\n",
    "\n",
    "#Training Ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f3207-5dbe-41ac-960d-f89cb515b973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
